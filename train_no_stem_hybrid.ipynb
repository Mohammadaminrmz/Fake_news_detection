{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorboard\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(vectors_ref:pd.DataFrame , word_dict:dict):\n",
    "    rows = len(word_dict) +1\n",
    "    columns = len(vectors_ref.columns)\n",
    "    shape = (rows , columns)\n",
    "    weights = np.zeros(shape=shape)\n",
    "    inx = list(set(vectors_ref.index) & set(list(word_dict.values())))\n",
    "    vectors_ref = vectors_ref.loc[inx]\n",
    "    for key in word_dict.keys():\n",
    "        try:\n",
    "            weights[key] = vectors_ref.loc[word_dict[key]]\n",
    "        except:\n",
    "            pass\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model(maxlen , weights , input_dim , output_dim = 100):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input((maxlen,)))\n",
    "    model.add(layers.Embedding(input_dim= input_dim , output_dim=100 , input_length= maxlen , weights = [weights],trainable = False))\n",
    "    model.add(layers.Conv1D(128,5)) \n",
    "    model.add(layers.MaxPool1D(pool_size=2))\n",
    "    model.add(layers.LSTM(32, activation='relu'))\n",
    "    model.add(layers.Dense(1 ,activation =\"sigmoid\"))\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(1e-2,decay_rate=0.9999,decay_steps=1000)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr_schedule),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_regex_no_stem.csv\")\n",
    "glove = pd.read_csv(\"glove.6B.100d.txt\",header=None , delimiter=\" \" ,quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "glove.set_index(0 , drop= True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data,  y_data = df[\"processed\"] , df[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_validation, y, y_validation = train_test_split(X_data, y_data, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = \"<oov>\", lower= True)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    514.000000\n",
       "mean     191.313230\n",
       "std      119.524566\n",
       "min        8.000000\n",
       "25%      111.250000\n",
       "50%      166.000000\n",
       "75%      232.000000\n",
       "max      861.000000\n",
       "Name: processed, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(tokenizer.index_word) +1\n",
    "longest_sentence_length = X_train.apply(lambda x : len(x.split())).max()\n",
    "# average_sentence_length = X_train.apply(lambda x : len(x.split())).mean()\n",
    "X_train.apply(lambda x : len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_sents = X_train.apply(lambda x : len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501945525291829"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len_of_sents < 300) / len(len_of_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_validation = tokenizer.texts_to_sequences(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen= maxlen , padding= \"post\")\n",
    "X_test = pad_sequences(X_test, maxlen= maxlen , padding= \"post\")\n",
    "X_validation = pad_sequences(X_validation, maxlen= maxlen , padding= \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = create_weights(glove , tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8019, 100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 200, 100)          801900    \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 196, 128)          64128     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 98, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 32)                20608     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 886,669\n",
      "Trainable params: 84,769\n",
      "Non-trainable params: 801,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(maxlen=maxlen , input_dim= input_dim , weights=weights )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 726831.4375 - accuracy: 0.5214\n",
      "Epoch 00001: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 9s 96ms/step - loss: 726831.4375 - accuracy: 0.5214 - val_loss: 10.7829 - val_accuracy: 0.5426\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 434.3632 - accuracy: 0.5292\n",
      "Epoch 00002: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 90ms/step - loss: 434.3632 - accuracy: 0.5292 - val_loss: 6.9808 - val_accuracy: 0.5581\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 8.4274 - accuracy: 0.4747\n",
      "Epoch 00003: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 90ms/step - loss: 8.4274 - accuracy: 0.4747 - val_loss: 19.1899 - val_accuracy: 0.4574\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 37.6591 - accuracy: 0.4981\n",
      "Epoch 00004: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 91ms/step - loss: 37.6591 - accuracy: 0.4981 - val_loss: 3.1931 - val_accuracy: 0.5426\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.3403 - accuracy: 0.5389\n",
      "Epoch 00005: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 91ms/step - loss: 1.3403 - accuracy: 0.5389 - val_loss: 2.2591 - val_accuracy: 0.4574\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.4963 - accuracy: 0.4903\n",
      "Epoch 00006: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 7s 109ms/step - loss: 2.4963 - accuracy: 0.4903 - val_loss: 0.7952 - val_accuracy: 0.5426\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.9813 - accuracy: 0.5039\n",
      "Epoch 00007: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 93ms/step - loss: 0.9813 - accuracy: 0.5039 - val_loss: 1.4240 - val_accuracy: 0.5504\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 14.9174 - accuracy: 0.5136\n",
      "Epoch 00008: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 95ms/step - loss: 14.9174 - accuracy: 0.5136 - val_loss: 0.8404 - val_accuracy: 0.5736\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.8551 - accuracy: 0.4922\n",
      "Epoch 00009: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 94ms/step - loss: 0.8551 - accuracy: 0.4922 - val_loss: 0.6902 - val_accuracy: 0.5659\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.6655 - accuracy: 0.5039Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "Epoch 00010: saving model to hybrid_model_weights\\cp.ckpt\n",
      "65/65 [==============================] - 6s 91ms/step - loss: 1.6655 - accuracy: 0.5039 - val_loss: 2.3781 - val_accuracy: 0.5581\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "log_dir = \"logs_lag_1\\\\\" + datetime.now().strftime(\"%d-%m-%Y_%H%M%S\")\n",
    "checkpoint_path = \"hybrid_model_weights/cp.ckpt\"\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "      monitor=\"accuracy\",\n",
    "      min_delta=1e-3,\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "\n",
    "check_point_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "history = model.fit(x = X_train , y = y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=8,\n",
    "                    callbacks=[early_stopping_cb,check_point_cb,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7287290b0480c475e07faa8d8f2642b950c35b54c7904c27a4cb3f03b8542e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
